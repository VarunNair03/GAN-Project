{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport cv2\nfrom tensorflow.keras.preprocessing.image import array_to_img\nfrom tensorflow.keras.callbacks import Callback\nimport warnings\nwarnings.filterwarnings('ignore')\n# Adam is going to be the optimizer for both\nfrom tensorflow.keras.optimizers import RMSprop\n# Binary cross entropy is going to be the loss for both \nfrom tensorflow.keras.losses import BinaryCrossentropy\nimport tensorflow as tf\nimport numpy as np\nfrom tensorflow import keras\nfrom keras import Sequential, layers\nfrom keras.layers import Conv2D, Dense, Flatten, Reshape, LeakyReLU, Dropout, UpSampling2D\nimport matplotlib.pyplot as plt \n# Brining in tensorflow datasets for fashion mnist \nimport tensorflow_datasets as tfds\n\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing import image\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.utils import image_dataset_from_directory\nfrom tensorflow.keras.models import Model\nimport pickle","metadata":{"execution":{"iopub.status.busy":"2024-03-23T18:08:20.322019Z","iopub.execute_input":"2024-03-23T18:08:20.322364Z","iopub.status.idle":"2024-03-23T18:08:37.297917Z","shell.execute_reply.started":"2024-03-23T18:08:20.322336Z","shell.execute_reply":"2024-03-23T18:08:37.297139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"strategy = tf.distribute.MirroredStrategy()\nprint('DEVICES AVAILABLE: {}'.format(strategy.num_replicas_in_sync))","metadata":{"execution":{"iopub.status.busy":"2024-03-23T18:08:37.299544Z","iopub.execute_input":"2024-03-23T18:08:37.300040Z","iopub.status.idle":"2024-03-23T18:08:38.352817Z","shell.execute_reply.started":"2024-03-23T18:08:37.300015Z","shell.execute_reply":"2024-03-23T18:08:38.351877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ds = tfds.load('fashion_mnist', split='train')","metadata":{"execution":{"iopub.status.busy":"2024-03-23T18:08:38.354065Z","iopub.execute_input":"2024-03-23T18:08:38.354727Z","iopub.status.idle":"2024-03-23T18:09:21.582322Z","shell.execute_reply.started":"2024-03-23T18:08:38.354691Z","shell.execute_reply":"2024-03-23T18:09:21.581432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataiterator = ds.as_numpy_iterator()","metadata":{"execution":{"iopub.status.busy":"2024-03-23T18:09:21.585184Z","iopub.execute_input":"2024-03-23T18:09:21.585857Z","iopub.status.idle":"2024-03-23T18:09:21.648219Z","shell.execute_reply.started":"2024-03-23T18:09:21.585828Z","shell.execute_reply":"2024-03-23T18:09:21.647502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(ncols=4, figsize=(20,20))\nfor idx in range(4): \n    sample = dataiterator.next()\n    ax[idx].imshow(np.squeeze(sample['image']), cmap = \"gray\")\n    ax[idx].title.set_text(sample['label'])","metadata":{"execution":{"iopub.status.busy":"2024-03-23T18:09:21.649181Z","iopub.execute_input":"2024-03-23T18:09:21.649420Z","iopub.status.idle":"2024-03-23T18:09:22.523505Z","shell.execute_reply.started":"2024-03-23T18:09:21.649393Z","shell.execute_reply":"2024-03-23T18:09:22.522527Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_training_data():\n    # Reload the dataset \n    train_dataset = tfds.load('fashion_mnist', split='train')\n#     print(len(train_dataset))\n    # Running the dataset through the scale_images preprocessing step\n    train_dataset = train_dataset.map(lambda x: x['image']/255) \n    # Cache the dataset for that batch \n    train_dataset = train_dataset.cache()\n    # Shuffle it up \n    train_dataset = train_dataset.shuffle(6000)\n    # Batch into 128 images per sample\n    train_dataset = train_dataset.batch(256)\n    \n    return train_dataset","metadata":{"execution":{"iopub.status.busy":"2024-03-23T18:09:22.524766Z","iopub.execute_input":"2024-03-23T18:09:22.525156Z","iopub.status.idle":"2024-03-23T18:09:22.531496Z","shell.execute_reply.started":"2024-03-23T18:09:22.525121Z","shell.execute_reply":"2024-03-23T18:09:22.530534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data=get_training_data()","metadata":{"execution":{"iopub.status.busy":"2024-03-23T18:09:22.532715Z","iopub.execute_input":"2024-03-23T18:09:22.533028Z","iopub.status.idle":"2024-03-23T18:09:22.611645Z","shell.execute_reply.started":"2024-03-23T18:09:22.533000Z","shell.execute_reply":"2024-03-23T18:09:22.610906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_data)","metadata":{"execution":{"iopub.status.busy":"2024-03-23T18:09:22.612667Z","iopub.execute_input":"2024-03-23T18:09:22.612947Z","iopub.status.idle":"2024-03-23T18:09:22.619126Z","shell.execute_reply.started":"2024-03-23T18:09:22.612923Z","shell.execute_reply":"2024-03-23T18:09:22.618124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.as_numpy_iterator().next().shape","metadata":{"execution":{"iopub.status.busy":"2024-03-23T18:09:22.620195Z","iopub.execute_input":"2024-03-23T18:09:22.620480Z","iopub.status.idle":"2024-03-23T18:09:22.995051Z","shell.execute_reply.started":"2024-03-23T18:09:22.620456Z","shell.execute_reply":"2024-03-23T18:09:22.994086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_generator():\n    model = tf.keras.Sequential()\n    model.add(layers.Dense(7*7*256, use_bias=False, input_shape=(28,)))\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n\n    model.add(layers.Reshape((7, 7, 256)))\n    assert model.output_shape == (None, 7, 7, 256)  # Note: None is the batch size\n\n    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n    assert model.output_shape == (None, 7, 7, 128)\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n\n    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n    assert model.output_shape == (None, 14, 14, 64)\n    model.add(layers.BatchNormalization())\n    model.add(layers.LeakyReLU())\n\n    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n    assert model.output_shape == (None, 28, 28, 1)\n    \n\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-03-23T18:09:22.998753Z","iopub.execute_input":"2024-03-23T18:09:22.999052Z","iopub.status.idle":"2024-03-23T18:09:23.008957Z","shell.execute_reply.started":"2024-03-23T18:09:22.999027Z","shell.execute_reply":"2024-03-23T18:09:23.008027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# with strategy.scope():\ngenerator = build_generator()\ngenerator.summary()","metadata":{"execution":{"iopub.status.busy":"2024-03-23T18:09:23.009994Z","iopub.execute_input":"2024-03-23T18:09:23.010267Z","iopub.status.idle":"2024-03-23T18:09:23.177279Z","shell.execute_reply.started":"2024-03-23T18:09:23.010245Z","shell.execute_reply":"2024-03-23T18:09:23.176425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.utils.plot_model(\n    generator,\n    to_file='/kaggle/working/generator.png',\n    show_shapes=False,\n    show_dtype=False,\n    show_layer_names=True,\n    rankdir='TB',\n    expand_nested=False,\n    dpi=96,\n    layer_range=None,\n    show_layer_activations=False,\n    show_trainable=False\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-23T18:09:23.178436Z","iopub.execute_input":"2024-03-23T18:09:23.178715Z","iopub.status.idle":"2024-03-23T18:09:23.464480Z","shell.execute_reply.started":"2024-03-23T18:09:23.178691Z","shell.execute_reply":"2024-03-23T18:09:23.463465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preds = model.predict(np.random.randn(32,3))\n\nimgs = generator.predict(tf.random.normal((4,28)))\n# print(imgs.shape)\n# Setup the subplot formatting \nfig, ax = plt.subplots(ncols=4, figsize=(20,20))\n# Loop four times and get images \nfor idx, img in enumerate(imgs): \n    # Plot the image using a specific subplot \n#     print(img.shape)\n    ax[idx].imshow(np.squeeze(img))\n    # Appending the image label as the plot title \n    ax[idx].title.set_text(idx)","metadata":{"execution":{"iopub.status.busy":"2024-03-23T18:09:23.465922Z","iopub.execute_input":"2024-03-23T18:09:23.466250Z","iopub.status.idle":"2024-03-23T18:09:26.789180Z","shell.execute_reply.started":"2024-03-23T18:09:23.466223Z","shell.execute_reply":"2024-03-23T18:09:26.788267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_discriminator(): \n#     with strategy.scope():\n    model = tf.keras.Sequential()\n    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n                                     input_shape=[28, 28, 1]))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3))\n\n    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n    model.add(layers.LeakyReLU())\n    model.add(layers.Dropout(0.3))\n\n    model.add(layers.Flatten())\n    model.add(layers.Dense(1))\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-03-23T18:09:26.790543Z","iopub.execute_input":"2024-03-23T18:09:26.790929Z","iopub.status.idle":"2024-03-23T18:09:26.798407Z","shell.execute_reply.started":"2024-03-23T18:09:26.790887Z","shell.execute_reply":"2024-03-23T18:09:26.797333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"discriminator = build_discriminator()\ndiscriminator.summary()","metadata":{"execution":{"iopub.status.busy":"2024-03-23T18:09:26.799701Z","iopub.execute_input":"2024-03-23T18:09:26.799991Z","iopub.status.idle":"2024-03-23T18:09:26.891441Z","shell.execute_reply.started":"2024-03-23T18:09:26.799967Z","shell.execute_reply":"2024-03-23T18:09:26.890548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf.keras.utils.plot_model(\n    discriminator,\n    to_file='/kaggle/working/discriminator.png',\n    show_shapes=False,\n    show_dtype=False,\n    show_layer_names=True,\n    rankdir='TB',\n    expand_nested=False,\n    dpi=96,\n    layer_range=None,\n    show_layer_activations=False,\n    show_trainable=False\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-23T18:09:26.892438Z","iopub.execute_input":"2024-03-23T18:09:26.892705Z","iopub.status.idle":"2024-03-23T18:09:26.972612Z","shell.execute_reply.started":"2024-03-23T18:09:26.892682Z","shell.execute_reply":"2024-03-23T18:09:26.971746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FashionGAN(Model): \n    def __init__(self, generator, discriminator, *args, **kwargs):\n        # Pass through args and kwargs to base class \n        super().__init__(*args, **kwargs)\n        \n        # Create attributes for gen and disc\n        self.generator = generator \n        self.discriminator = discriminator \n        self.g_losses, self.d_losses = [], []\n        self.g_loss_tracker = keras.metrics.Mean(name = \"g_loss\")\n        self.d_loss_tracker = keras.metrics.Mean(name = \"d_loss\")\n        \n    def compile(self, g_opt, d_opt, g_loss, d_loss, *args, **kwargs): \n        # Compile with base class\n        super().compile(*args, **kwargs)\n        \n        # Create attributes for losses and optimizers\n        self.g_opt = g_opt\n        self.d_opt = d_opt\n        self.g_loss = g_loss\n        self.d_loss = d_loss \n\n    def train_step(self, batch):\n        # Get the data \n        real_images = batch\n        fake_images = self.generator(tf.random.normal((256, 28,1)), training=False)\n        \n        # Train the discriminator\n        with tf.GradientTape() as d_tape: \n            # Pass the real and fake images to the discriminator model\n            yhat_real = self.discriminator(real_images, training=True) \n            yhat_fake = self.discriminator(fake_images, training=True)\n            yhat_realfake = tf.concat([yhat_real, yhat_fake], axis=0)\n\n            # Create labels for real and fakes images\n            y_realfake = tf.concat([tf.zeros_like(yhat_real), tf.ones_like(yhat_fake)], axis=0)\n\n            # Add some noise to the TRUE outputs\n            noise_real = 0.15*tf.random.uniform(tf.shape(yhat_real))\n            noise_fake = -0.15*tf.random.uniform(tf.shape(yhat_fake))\n            y_realfake += tf.concat([noise_real, noise_fake], axis=0)\n\n            # Calculate loss - BINARYCROSS \n            total_d_loss = self.d_loss(y_realfake, yhat_realfake)\n        \n        \n        \n        # Apply backpropagation - nn learn \n        dgrad = d_tape.gradient(total_d_loss, self.discriminator.trainable_variables) \n        self.d_opt.apply_gradients(zip(dgrad, self.discriminator.trainable_variables))\n\n        # Train the generator \n        with tf.GradientTape() as g_tape: \n            # Generate some new images\n            gen_images = self.generator(tf.random.normal((256,28,1)), training=True)\n\n            # Create the predicted labels\n            predicted_labels = self.discriminator(gen_images, training=False)\n\n            # Calculate loss - trick to training to fake out the discriminator\n            total_g_loss = self.g_loss(tf.zeros_like(predicted_labels), predicted_labels) \n        \n\n        # Apply backprop\n        ggrad = g_tape.gradient(total_g_loss, self.generator.trainable_variables)\n        self.g_opt.apply_gradients(zip(ggrad, self.generator.trainable_variables))\n        \n        self.g_loss_tracker.update_state(total_g_loss)\n        self.d_loss_tracker.update_state(total_d_loss)\n        return {\n            \"d_loss\":self.d_loss_tracker.result(), \n            \"g_loss\":self.g_loss_tracker.result()\n        }\n    @property\n    def metrics(self):\n        return [ self.d_loss_tracker, self.g_loss_tracker]","metadata":{"execution":{"iopub.status.busy":"2024-03-23T18:09:26.973694Z","iopub.execute_input":"2024-03-23T18:09:26.973955Z","iopub.status.idle":"2024-03-23T18:09:26.990262Z","shell.execute_reply.started":"2024-03-23T18:09:26.973932Z","shell.execute_reply":"2024-03-23T18:09:26.989242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ModelMonitor(Callback):\n    def __init__(self, num_img=3, latent_dim=28):\n        self.num_img = num_img\n        self.latent_dim = latent_dim\n        \n\n    def on_epoch_end(self, epoch, logs=None):\n        if epoch % 5 == 0:\n            # saving the model weights\n            self.model.generator.save_weights(\"/kaggle/working/F_GANgen.weights.h5\")\n            self.model.discriminator.save_weights(\"/kaggle/working/F_GANdis.weights.h5\")\n            \n            # saving the generator optimizer config\n            gen_config = self.model.g_opt.get_config()\n            with open('/kaggle/working/g_optimizer.pkl', 'wb') as f:\n                pickle.dump(gen_config, f)\n                \n            # saving the discriminator optimizer config\n            des_config = self.model.d_opt.get_config()\n            with open('/kaggle/working/d_optimizer.pkl', 'wb') as f:\n                pickle.dump(des_config,f)\n            \n            # saving the generator and descriminator\n#             with open('/kaggle/working/desc.pkl', 'wb') as f:\n#                 pickle.dump(self.model.discriminator, f)\n                \n#             with open('/kaggle/working/gen.pkl', 'wb') as f:\n#                 pickle.dump(self.model.generator, f)\n\n            self.model.generator.save('/kaggle/working/gen.keras')\n            self.model.discriminator.save('/kaggle/working/des.keras')\n\n                \n                \n        if epoch % 50 == 0:\n            random_latent_vectors = tf.random.uniform((self.num_img, self.latent_dim,1))\n            generated_images = self.model.generator(random_latent_vectors)\n            generated_images *= 255\n            generated_images.numpy()\n            \n            for i in range(self.num_img):\n                img = array_to_img(generated_images[i])\n                img.save(os.path.join('/kaggle/working/', f'generated_img_{epoch}_{i}.png'))","metadata":{"execution":{"iopub.status.busy":"2024-03-23T18:09:26.991335Z","iopub.execute_input":"2024-03-23T18:09:26.991594Z","iopub.status.idle":"2024-03-23T18:09:27.003848Z","shell.execute_reply.started":"2024-03-23T18:09:26.991573Z","shell.execute_reply":"2024-03-23T18:09:27.003082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    # Define learning rates\n    g_learning_rate = 1e-5\n    d_learning_rate = 1e-6\n\n    # Create RMSprop optimizers for generator and discriminator\n    g_opt = RMSprop(learning_rate=g_learning_rate)\n    d_opt = RMSprop(learning_rate=d_learning_rate)\n    g_loss = BinaryCrossentropy()\n    d_loss = BinaryCrossentropy()\n    generator = build_generator()\n    discriminator = build_discriminator()\n    f_GAN = FashionGAN(generator, discriminator)\n    \n    f_GAN.compile(g_opt, d_opt, g_loss, d_loss)\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-03-23T18:09:27.004921Z","iopub.execute_input":"2024-03-23T18:09:27.005211Z","iopub.status.idle":"2024-03-23T18:09:27.259689Z","shell.execute_reply.started":"2024-03-23T18:09:27.005189Z","shell.execute_reply":"2024-03-23T18:09:27.258909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hist = f_GAN.fit(train_data, epochs=2000, callbacks=[ModelMonitor()])","metadata":{"execution":{"iopub.status.busy":"2024-03-23T18:09:27.260791Z","iopub.execute_input":"2024-03-23T18:09:27.261066Z","iopub.status.idle":"2024-03-23T18:09:53.001613Z","shell.execute_reply.started":"2024-03-23T18:09:27.261042Z","shell.execute_reply":"2024-03-23T18:09:53.000146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def PlotAccuracy(net):\n    plt.figure(figsize=(15, 4))\n    plt.subplot(1,3,1)\n    plt.plot(net['d_loss'],label='Discriminator Loss')\n    plt.plot(net['g_loss'],label='Generator Loss')\n    plt.title(\"RMSProp Discriminator and Generator Loss\")\n    plt.ylabel('Loss')\n    plt.xlabel('epochs')\n    plt.legend()\n    plt.plot()\n\n\n    plt.subplot(1,3,2)\n    plt.plot(net['d_loss'],label='Discriminator Loss')\n    plt.plot(net['g_loss'],label='Generator Loss')\n    plt.title(\"Adam Discriminator and Generator Loss\")\n    plt.ylabel('Loss')\n    plt.xlabel('epochs')\n    plt.legend()\n    plt.plot()\n\nPlotAccuracy(hist.history)","metadata":{"execution":{"iopub.status.busy":"2024-03-23T18:09:53.003458Z","iopub.execute_input":"2024-03-23T18:09:53.004550Z","iopub.status.idle":"2024-03-23T18:09:53.499244Z","shell.execute_reply.started":"2024-03-23T18:09:53.004512Z","shell.execute_reply":"2024-03-23T18:09:53.498345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(\"/kaggle/working/Adamaxhistory.pkl\",\"wb\") as f:\n    pickle.dump(hist.history,f)","metadata":{"execution":{"iopub.status.busy":"2024-03-23T18:09:53.500610Z","iopub.execute_input":"2024-03-23T18:09:53.500994Z","iopub.status.idle":"2024-03-23T18:09:53.506164Z","shell.execute_reply.started":"2024-03-23T18:09:53.500960Z","shell.execute_reply":"2024-03-23T18:09:53.505143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# to be run after the model is trained\nimgs = generator(tf.random.normal((16, 28)))\n\nfig, ax = plt.subplots(ncols=4, nrows=4, figsize=(10,10))\nfor r in range(4): \n    for c in range(4): \n        ax[r][c].imshow(imgs[(r+1)*(c+1)-1])\n","metadata":{"execution":{"iopub.status.busy":"2024-03-23T18:09:53.507388Z","iopub.execute_input":"2024-03-23T18:09:53.508033Z","iopub.status.idle":"2024-03-23T18:09:55.850121Z","shell.execute_reply.started":"2024-03-23T18:09:53.508001Z","shell.execute_reply":"2024-03-23T18:09:55.849112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# finally saving the generator and descriminator models\n\ngenerator.save(\"/kaggle/working/final_generator_model.keras\")\ndiscriminator.save(\"/kaggle/working/final_descriminator_model.keras\")","metadata":{"execution":{"iopub.status.busy":"2024-03-23T18:09:55.851645Z","iopub.execute_input":"2024-03-23T18:09:55.852327Z","iopub.status.idle":"2024-03-23T18:09:55.909495Z","shell.execute_reply.started":"2024-03-23T18:09:55.852289Z","shell.execute_reply":"2024-03-23T18:09:55.908738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### If the model has to be trained again using the saved weights, we run the following code\n","metadata":{}},{"cell_type":"code","source":"# with strategy.scope():\n#     gen, des = build_generator(), build_discriminator()\n#     gen.load_weights(\"/kaggle/input/model-attributes/F_GANgen.weights.h5\")\n#     des.load_weights(\"/kaggle/input/model-attributes/F_GANdis.weights.h5\")\n#     newModel = FashionGAN(gen,des)\n# #     print(newModel.d_losses,newModel.g_losses)\n#     # loading the saved weights\n#     with open(\"/kaggle/input/model-attributes/g_optimizer (1).pkl\", \"rb\") as fp:\n#         g = pickle.load(fp)\n#     with open(\"/kaggle/input/model-attributes/d_optimizer (1).pkl\", \"rb\") as fp:\n#         d = pickle.load(fp)\n        \n# #     newModel.load_weights('/kaggle/input/old-modeld/F_GAN.weights.h5')\n    \n#     # re-assigning the previous weights\n#     new_gopt = Adamax().from_config(g)\n#     new_dopt = Adamax().from_config(d)\n#     g_loss = BinaryCrossentropy()\n#     d_loss = BinaryCrossentropy()\n#     newModel.compile(new_gopt, new_dopt,g_loss,d_loss) \n    ","metadata":{"execution":{"iopub.status.busy":"2024-03-23T18:09:55.910505Z","iopub.execute_input":"2024-03-23T18:09:55.910769Z","iopub.status.idle":"2024-03-23T18:09:55.915142Z","shell.execute_reply.started":"2024-03-23T18:09:55.910745Z","shell.execute_reply":"2024-03-23T18:09:55.914248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hist = newModel.fit(train_data, epochs = 6, callbacks=[ModelMonitor()])\n","metadata":{"execution":{"iopub.status.busy":"2024-03-23T18:09:55.916295Z","iopub.execute_input":"2024-03-23T18:09:55.916631Z","iopub.status.idle":"2024-03-23T18:09:55.929032Z","shell.execute_reply.started":"2024-03-23T18:09:55.916602Z","shell.execute_reply":"2024-03-23T18:09:55.928126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.plot(hist.history[\"loss\"])\n# hist.history","metadata":{"execution":{"iopub.status.busy":"2024-03-23T18:09:55.930014Z","iopub.execute_input":"2024-03-23T18:09:55.930297Z","iopub.status.idle":"2024-03-23T18:09:55.937916Z","shell.execute_reply.started":"2024-03-23T18:09:55.930276Z","shell.execute_reply":"2024-03-23T18:09:55.936887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# imgs = gen(tf.random.normal((16, 28)))\n\n# fig, ax = plt.subplots(ncols=4, nrows=4, figsize=(10,10))\n# for r in range(4): \n#     for c in range(4): \n#         ax[r][c].imshow(imgs[(r+1)*(c+1)-1])","metadata":{"execution":{"iopub.status.busy":"2024-03-20T14:20:21.999251Z","iopub.execute_input":"2024-03-20T14:20:21.999649Z","iopub.status.idle":"2024-03-20T14:20:24.734121Z","shell.execute_reply.started":"2024-03-20T14:20:21.999602Z","shell.execute_reply":"2024-03-20T14:20:24.733077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}